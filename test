---
title: "Predictive Analytics"
subtitle: "Linear Model"  
author: "Zhiyu (Frank) Quan"
institute: "University of Illinois Urbana-Champaign"
#date: "2022/04/01 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    nature:
      slideNumberFormat: "%current%"
      highlightStyle: github
      highlightLines: true
      ratio: 16:9
      countIncrementalSlides: true
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  fig.width=9, fig.height=3.5, fig.retina=3,
  out.width = "100%",
  cache = FALSE,
  echo = TRUE,
  message = FALSE, 
  warning = FALSE,
  hiline = TRUE
)
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_duo_accent(
  primary_color = "#13294B",
  secondary_color = "#DD3403",
  inverse_header_color = "#00000",
  text_font_size = "27px",
)

```




```{r, include=FALSE}

RNGkind(sample.kind = "Rounding") # Uncomment if running R 3.6.x or later

# Load packages and data

# Load packages
library(plyr)
library(dplyr)
library(ggplot2)

# Load data
data_all <- read.csv(file = "June 16 data.csv")

data_all$admit_type_id <- as.factor(data_all$admit_type_id)

cat_vars <- c("gender", "age", "race", 
              "weight", "admit_type_id", 
              "metformin", "insulin", "readmitted")

num_vars <- c("num_procs", "num_meds", "num_ip", 
              "num_diags")

data_all <- subset(data_all, gender != "Unknown/Invalid")

data_all$gender <- as.factor(data_all$gender)

data_all$race <- as.factor(data_all$race)
var.levels <- levels(data_all$race) 
var.levels
data_all$race <- mapvalues(
  data_all$race, var.levels,
  c("Missing","AfricanAmerican", "Other", "Caucasian", "Other", "Other")
)
rm(var.levels)

data_all$weight <- NULL

cat_vars = cat_vars[!cat_vars %in% c("weight")]

data_all = data_all %>% mutate_if(is.character, as.factor)

vars <- c("gender", "age", "race", 
          "metformin", "insulin", 
          "readmitted", "admit_type_id") 
for (i in vars) {
  table <- as.data.frame(table(data_all[, i]))
  max <- which.max(table[, 2])
  level.name <- as.character(table[max, 1])
  data_all[, i] <- relevel(data_all[, i], ref = level.name)
}
rm(vars)

```



### Linear Regression Model

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

- Linear regression is a simple approach to supervised learning. It assumes that response variable $Y$ is a linear combination of corresponding explanatory variable $X_1, \dots, X_p$.
  - Data set: $(X_i,Y_i)$, for $i=1,2,\ldots,n$
  - $(X_i,Y_i)$ are **independent** and **identical** copies of $(X,Y)$
- True regression functions are never linear!
- Although it may seem overly simplistic, linear regression is extremely useful both conceptually and practically.

---

### Simple Linear Regression using a Single Predictor $X$

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

- We assume there is a linear relationship between $X_i$ and $Y_i$:
$$
Y_i=\beta_0+\beta_1X_i+\epsilon_i
$$
where $\beta_0$ and $\beta_1$ are two unknown constants that represent the **intercept** and **slope**, also known as **coefficients** or **parameters**, and $\epsilon_i$ is the **error term**.
- $\epsilon_i$ are i.i.d. random variables
  - $\mathbb{E}[\epsilon_i]=0$ and $\text{Var}[\epsilon_i]=\sigma^2.$
    - homoscedasticity: the variance is independent of $i$
  - Convenient assumption: $\epsilon_i\overset{d}{=}\mathcal{N}\left(0,\sigma^2 \right)$
- Available Data: $\mathcal{D}=\left\{(x_i,y_i)|i=1,2,\ldots,n\right\}.$
  - $y_i$ is a realization (or observation) of the response variable $Y_i$
  - $x_i$ is the observed value of the explanatory variable $X_i$.

---


### Simple Linear Regression Prediction

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

- Given some estimates $\hat \beta_0$ and $\hat \beta_1$ for the model coefficients, we predict future $Y$ using 
$$
\hat{y}_i=\hat\beta_0+ \hat\beta_1x_i
$$
where $\hat{y}_i$ indicates a prediction of $Y$ on the basis of $X = x_i$. The hat symbol denotes an estimated value.
- Inference: 
  - If $x$ increases to $x+\Delta_x$, then the effect on $y$ is $\Delta_y=\hat{\beta}_1\Delta_x$
- Calibrating/estimating the model = Approximating $\hat \beta_0$ and $\hat \beta_1$.

---

### Estimation of the Parameters by Least Squares

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

- **Residual**: $e_i = {y}_i - \hat{y}_i$
  - error
- Method of Least squares: estimate parameters $\beta_0$ and $\beta_1$
  - by minimizing the distance between the observations $y_i$ and the predictions $\hat{y}_i$
  - using the sum of squares
- We define the **residual sum of squares** ( $RSS(\beta_0,\beta_1)$ ) as $RSS(\beta_0,\beta_1) = e_1^2+e_2^2+ \dots + e_n^2$ or equivalently as 

$$RSS(\beta_0,\beta_1) = (y_1-(\beta_0+\beta_1x_1))^2 + (y_2-(\beta_0+\beta_1x_2))^2 + \\ \dots + (y_n-(\beta_0+\beta_1x_n))^2$$

---

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

- Simply with fixed $\hat\beta_0$ and $\hat\beta_1$

$$RSS(\hat\beta_0,\hat\beta_1) = \sum_{i=1}^{n} \left(y_i-\hat{y}_i \right)^2 = \sum_{i=1}^{n} \left(y_i-(\hat\beta_0+\hat\beta_1x_i) \right)^2$$

- The least squares approach finds $\hat\beta_0$ and $\hat\beta_1$ to minimize the $RSS(\beta_0,\beta_1)$, i.e.:

$$\left(\hat{\beta}_0,\hat{\beta}_1 \right)=\arg \min_{\left( \beta_0,\beta_1\right)} RSS\left(\beta_0,\beta_1 \right).$$

---

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

- Solve the following equations and find out $\hat\beta_0$ and $\hat\beta_1$:

$$\left\{
      \begin{array}{l}
        \frac{\partial RSS\left(\beta_0,\beta_1 \right)}{\partial \beta_0}=0, \\
        \frac{\partial RSS\left(\beta_0,\beta_1 \right)}{\partial \beta_1}=0.
      \end{array}
    \right.$$

- And verify the Multivariable Second Derivative Test (Second Partial Derivative Test)
  
$$\left\{
      \begin{array}{l}
        \frac{\partial^2 RSS\left(\beta_0,\beta_1 \right)}{\partial \beta_0^2}> 0, \\
        \frac{\partial^2 RSS\left(\beta_0,\beta_1 \right)}{\partial \beta_1^2}> 0.\\
 \frac{\partial^2 RSS\left(\beta_0,\beta_1 \right)}{\partial \beta_0^2} \frac{\partial^2 RSS\left(\beta_0,\beta_1 \right)}{\partial \beta_1^2}-\left( \frac{\partial^2 RSS\left(\beta_0,\beta_1 \right)}{\partial \beta_0\beta_1} \right)^2> 0.
      \end{array}
    \right.$$

---

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

- The minimizing values can be shown to be
$$
\begin{eqnarray*}
  \hat \beta_1 &=& \frac{ \sum_{i=1}^{n}(x_i-\overline{x})(y_i-\overline{y}) }{\sum_{i=1}^{n}(x_i-\overline{x})^2 }\\
  \hat \beta_0 &=& \overline{y}- \hat \beta_1 \overline{x}
\end{eqnarray*}
$$
where $\overline{y} = \frac{1}{n}\sum^{n}_{i=1} y_i$ and $\overline{x} = \frac{1}{n}\sum^{n}_{i=1} x_i$ are the sample means.
  - Given the data set $(x_i,y_i),\ i=1,2,\ldots,n$, we can calculate $\hat\beta_0$ and $\hat\beta_1$

---

### Correlation and Simple Linear Regression

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

- Correlation analysis is a technique used to quantify the associations between two continuous variables.
  - We estimate a **sample correlation coefficient**, more specifically the **Pearson Product Moment correlation coefficient**. The sample correlation coefficient, denoted $r$.

---

#### Pearson Correlation

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

- The Pearson correlation has two assumptions:
  - The two variables are normally distributed. We can test this assumption using
      - A statistical test (Shapiro-Wilk)
      - A histogram
      - A QQ plot
  - The relationship between the two variables is linear. If this relationship is found to be curved, etc. we need to use another correlation test. We can test this assumption by examining the scatterplot between the two variables.

---

```{r}
x <- seq(-10,10, 1)
y <- x*x
plot(x,y)
```

---

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

```{r}
cor(x, y, method="pearson")
rm(x, y)
```

---

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

- $r$ ranges between -1 and +1 and quantifies the direction and strength of the linear association between the two variables. The correlation between two variables can be positive (i.e., higher levels of one variable are associated with higher levels of the other) or negative (i.e., higher levels of one variable are associated with lower levels of the other).
- The sign of the correlation coefficient indicates the direction of the association. The magnitude of the correlation coefficient indicates the strength of the association.
- A correlation close to zero suggests no linear association between two continuous variables.

---

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

- There may be a non-linear association between two continuous variables, but the computation of a correlation coefficient does not detect this. Therefore, it is always important to evaluate the data carefully before computing a correlation coefficient. Graphical displays are handy for exploring associations between variables.

---

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

- The formula for the sample correlation coefficient is:
$$r = \frac{Cov(X,Y)}{\sqrt{s^2_Xs^2_Y}}$$
where $Cov(X,Y)$ is the covariance of X and Y defined as
$$Cov(X,Y) =  \frac{ \sum_{i=1}^{n}(x_i-\overline{x})(y_i-\overline{y}) }{n-1}$$
$s^2_X$ and $s^2_Y$ are the sample variances of X and Y, defined as follows:
$$s^2_X= \frac{\sum_{i=1}^{n}(x_i-\overline{x})^2}{n-1} \quad and \quad s^2_Y= \frac{\sum_{i=1}^{n}(y_i-\overline{y})^2}{n-1}$$

---

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

- Regression analysis is a related technique to assess the relationship between a response variable and one or more explanatory variables (risk factors).
$$\begin{eqnarray*}
  \beta_1 &=& \frac{ \sum_{i=1}^{n}(x_i-\overline{x})(y_i-\overline{y}) }{\sum_{i=1}^{n}(x_i-\overline{x})^2 }\\
   &=& \frac{\frac{1}{(n-1)s_Xs_Y} \sum_{i=1}^{n}(x_i-\overline{x})(y_i-\overline{y}) }{\frac{1}{(n-1)s_Xs_Y}\sum_{i=1}^{n}(x_i-\overline{x})^2 }\\
   &=& \frac{\frac{1}{s_Xs_Y}Cov(X,Y)}{\frac{1}{s_Xs_Y}s^2_X}. \\
&=& r\frac{s_Y}{s_X}.
\end{eqnarray*}$$

---

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

```{r}
beta1 <- cor(data_all$num_meds, 
             data_all$days, 
             method="pearson")*sd(data_all$days)/sd(data_all$num_meds)
beta1
beta0 <- mean(data_all$days)-beta1*mean(data_all$num_meds)
beta0
slm <- lm(formula = days ~ num_meds, data = data_all)
slm
```

---

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

```{r}
cor(data_all$num_meds, data_all$days, method="pearson")
```

---

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

If $r$, the relationship, is statistically significantly different from $0$
 - $H_0$: There is no correlation between the two variables: $r = 0$
 - $H_a$: There is a nonzero correlation between the two variables: $r \ne 0$
 
```{r}
cor.test(data_all$num_meds, data_all$days, method="pearson")
```

---

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

When testing the null hypothesis that there is no correlation between `num_meds` and `days`, we reject the null hypothesis (r = 0.4722805, t = 53.567, with 9995 degrees of freedom, and a p-value < 2.2e-16). As `num_meds` increases so does `days`. The 95% confidence interval for the correlation between `num_meds` and `days` is (0.4569076, 0.4873714). Note that this 95% confidence interval does not contain 0, which is consistent with our decision to reject the null hypothesis.

---

#### Spearman's Rank Correlation

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

Spearman's rank correlation is a nonparametric measure of the correlation that uses the rank of observations in its calculation, rather than the original numeric values. It measures the monotonic relationship between two variables X and Y. That is, 
- if Y tends to increase as X increases, the Spearman correlation coefficient is positive. 
- if Y tends to decrease as X increases, the Spearman correlation coefficient is negative. 
- A value of zero indicates that there is no tendency for Y to either increase or decrease when X increases. 

---

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

- The Spearman correlation measurement makes no assumptions about the distribution of the data.

- The formula for Spearman's correlation $\rho_s$ is
$$\rho_s = 1 - \frac{6\sum_{i=1}^nd_i^2}{n(n^2-1)}$$
where $d_i$ is the difference in the ranked observations from each group, $(x_i-y_i)$, and $n$ is the sample size.

---

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

```{r}
x <- seq(-10,10, 1)
y <- x*x
cor(x, y, method="pearson")
cor(x, y, method="spearman")
rm(x, y) 
```

---

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

```{r}
cor(data_all$num_meds, data_all$days, method="spearman")
```


```{r}
cor.test(data_all$num_meds, data_all$days, method="spearman")
```

Thus we reject the null hypothesis that there is no (Spearman) correlation between `num_meds` and `days` (r = 0.4680778, p-value < 2.2e-16 ). As `num_meds` increases so does `days`.

---

### Confidence Intervals for Coefficient Estimates

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

- The standard error of an estimator reflects how it varies under repeated sampling. We have
$$SE(\hat\beta_1)^2 = \frac{\sigma^2}{\sum_{i=1}^{n}(x_i-\overline{x})^2}$$

$$SE(\hat\beta_0)^2 = \sigma^2 ( \frac{1}{n}+\frac{\overline{x}^2}{\sum_{i=1}^{n}(x_i-\overline{x})^2})$$
where $\sigma^2 = Var(\epsilon)$

Why?

---

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

- The standard error becomes smaller if $n$ becomes larger.
  - More data leads to more accuracy.
- The standard error becomes smaller if $s^2_X = \frac{\sum_{i=1}^{n}(x_i-\overline{x})^2}{n-1}$ becomes larger.
  - Larger spread of the $X$ values leads to higher accuracy of the point estimates $\hat{\beta}_0$ and $\hat{\beta}_1$.
- The standard error becomes smaller if $\sigma^2$ becomes smaller.
  - Smaller spread of the $y$ values leads to higher accuracy of the point estimates $\hat{\beta}_0$ and $\hat{\beta}_1$

---

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

- These standard errors can be used to compute **confidence intervals**. A 95% confidence interval is defined as a range of values such that with 95% probability, the range will contain the true unknown value of the parameter. It has the form

$$
\hat\beta_1 \pm 2 SE(\hat\beta_1)
$$

- That is, there is approximately a 95% chance that the interval will contain the true value of $\beta_1$ (under a scenario where we got repeated samples like the present sample)

```{r}
confint(slm)
```

---

### Hypothesis Testing

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

- Standard errors can also be used to perform hypothesis tests on the coefficients. The most common hypothesis test involves testing the null hypothesis of
  - $H_0$: There is no relationship between X and Y
  - $H_a$: There is some relationship between X and Y 
- Mathematically, this corresponds to testing
  - $H_0$: $\beta_1=0$
  - $H_a$: $\beta_1 \ne 0$
- since if $\beta_1=0$ then the model reduces to $Y = \beta_0 + \epsilon$, and X is not associated with Y.

---

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

- To test the null hypothesis, we compute a **t-statistic**, given by

$$
t = \frac{\hat\beta_1 - 0 }{SE(\hat\beta_1)}
$$

- This will have a t-distribution with n − 2 degrees of freedom, assuming $\beta_1=0$.

- Assume the random variable $T_{n-2}$ has a $t$-distribution with $n-2$ degrees of freedom.

- The quantile $t_{n-2,1-\frac{\alpha}{2}}$ is defined as follows:

$$\mathbb{P}\left[T_{n-2}\leq  t_{n-2,1-\frac{\alpha}{2}}\right]=1-\frac{\alpha}{2}$$

---

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

- Since the distribution of $T_{n-2}$ is symmetric around zero:

$$\mathbb{P}\left[T_{n-2}>  |t_{n-2,1-\frac{\alpha}{2}}|\right]=\alpha$$

- quantiles of a $t$ distribution: `qt(p, df)` in `R` 

- Assume the null hypthesis is true, then: $t\left( \hat{\beta}_1\right)$ is a realisation from a $t$-distribution. We expect the $t$-ratio to be in the interval:

$$\left[ -t_{n-2,1-\frac{\alpha}{2}},t_{n-2,1-\frac{\alpha}{2}}\right]$$
which is equivalent with saying that we expect that:
$$t\left( \hat{\beta}_1\right)< |t_{n-2,1-\frac{\alpha}{2}}|$$

---

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

- Conclusion
$$\text{reject the null hypothesis if }$$
$$t\left( \hat{\beta}_1\right)>|t_{n-2,1-\frac{\alpha}{2}}|$$
---

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

- $p$-value:

$$\mathbb{P}\left[T_{n-2}> |t\left(\hat{\beta_1} \right)| \right]=p$$

- 
  - If $p$ is large, then the probability that we observe $t\left(\hat{\beta_1}\right)$ is likely
  - If $p$ is small (e.g. 1 %), then it is very unlikely that $t\left(\hat{\beta_1}\right)$ comes from the distribution $T_{n-2}$
  - Calculate the $p$-value:

$$\begin{eqnarray*}
  p &=& 1-\mathbb{P}\left[-t\left(\hat{\beta_1}\right)\leq T_{n-2}\leq t\left(\hat{\beta_1}\right) \right] \\
   &=& 1-\left(F_{T_{n-2}}\left(t\left(\hat{\beta_1}\right) \right)-F_{T_{n-2}}\left(-t\left(\hat{\beta_1} \right)\right) \right).
\end{eqnarray*}$$

---

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

- Assume: $\beta_1\neq 0$ and we hope: $\hat{\beta}_1\approx \beta_1$. We have 

$$\frac{\hat{\beta}_1-\beta_1}{se\left( \hat{\beta}_1\right)}=T_{n-2}$$

- Then:

$$\mathbb{P}\left[ -t_{n-2,1-\frac{\alpha}{2}}\leq \frac{\hat{\beta}_1-\beta_1}{se\left( \hat{\beta}_1\right)} \leq t_{n-2,1-\frac{\alpha}{2}} \right]=1-\alpha$$

---

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

- Confidence interval for $\beta_1$ with confidence level $1-\alpha$:

$$\left[\hat{\beta}_1-se\left( \hat{\beta}_1\right)t_{n-2,1-\frac{\alpha}{2}},  \hat{\beta}_1+se\left( \hat{\beta}_1\right)t_{n-2,1-\frac{\alpha}{2}}\right]$$

- $t$-ratios

$$\begin{eqnarray*}
  t\left(\hat{\beta}_0 \right) &=& \frac{\hat{\beta}_0}{se\left( \hat{\beta}_0\right)} \\
  t\left(\hat{\beta}_1 \right) &=& \frac{\hat{\beta}_1}{se\left( \hat{\beta}_1\right)}.
\end{eqnarray*}$$


---

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

- $p$-values

$$\begin{eqnarray*}
   p\left(\hat{\beta}_0 \right) &=&  1-\left(F_{T_{n-2}}\left(t\left(\hat{\beta_0}\right) \right)-F_{T_{n-2}}\left(-t\left(\hat{\beta_0} \right)\right) \right)\\
   p\left(\hat{\beta}_1 \right) &=& 1-\left(F_{T_{n-2}}\left(t\left(\hat{\beta_1}\right) \right)-F_{T_{n-2}}\left(-t\left(\hat{\beta_1} \right)\right) \right).
\end{eqnarray*}$$

- Confidence Intervals

$$\begin{eqnarray*}
\beta_0&\in &\left[\hat{\beta}_0-se\left( \hat{\beta}_0\right)t_{n-2,1-\frac{\alpha}{2}},  \hat{\beta}_0+se\left( \hat{\beta}_0\right)t_{n-2,1-\frac{\alpha}{2}}\right]\\
\beta_1&\in&\left[\hat{\beta}_1-se\left( \hat{\beta}_1\right)t_{n-2,1-\frac{\alpha}{2}},  \hat{\beta}_1+se\left( \hat{\beta}_1\right)t_{n-2,1-\frac{\alpha}{2}}\right].
\end{eqnarray*}$$

---

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

- Using statistical software, it is easy to compute the probability of observing any value equal to |t| or larger. We call this probability the p-value.

```{r}
summary(slm)
```


---

### Assessing the Overall Accuracy of the Model

```{r}
ggplot(data_all, aes(x=num_meds, y=days)) +
  geom_point(shape=1) + geom_smooth(method=lm)  
```

---

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

```{r}
predict(slm,data.frame(num_meds=c(0,20,40,60)),interval="confidence")
```

---

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

```{r}
data_all %>% filter(num_meds == 0) %>% 
  summarise(mean = mean(days))
data_all %>% filter(num_meds == 20) %>% 
  summarise(mean = mean(days))
data_all %>% filter(num_meds == 40) %>% 
  summarise(mean = mean(days))
data_all %>% filter(num_meds == 60) %>% 
  summarise(mean = mean(days))
```


---

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

- Observation: The predicted values $\hat{y}_i$ do not exactly match the observed values $y_i$
- Explanation: The prediction $\hat{y}_i$ approximates the systematic part of the regression model:

$$
\hat{y}_i \approx f(x_i)=\hat\beta_0+\hat\beta_1x_i
$$

- The prediction $\hat{y}_i$ approximates the expected response:

$$
\hat{y}_i \approx \mathbb{E}[Y_i|\ X_i=x_i]
$$

---

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

- The **total sum of squares (TSS)**. We do not use $X$ to predict the response $Y$. Best estimator: $\overline{y}$ then
$$TSS=\sum_{i=1}^{n}\left(y_i-{\color{red}{\overline{y}}} \right)^2.$$


  - Small TSS = low variability in the responses.
    - estimating responses with $\overline{y}$ works well. Mean Works.
    - there is no (urgent) need for an advanced prediction model.
  
  - High TSS = high variability in the responses
    - large deviations can be expected when using  $\overline{y}$ 

---

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

- The **residual sum of squares (RSS)**

$$RSS=\sum_{i=1}^{n}\left(y_i-{\color{red}{\hat{y}_i}} \right)^2$$

- We can write:

$$\left(y_i-\overline{y}\right)= \left(y_i-\hat{y}_i \right) +\left( \hat{y}_i-\overline{y}\right)$$

  - $\left(y_i-\overline{y}\right)$: Deviation of the estimate $\overline{y}$ from the observation $y_i$
  - $\left(y_i-\hat{y}_i \right)$: Deviation of the estimate $\hat{y}_i$ from the observation $y_i$
  - $\left( \hat{y}_i-\overline{y}\right)$: Deviation explained by the regression line

---

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

- Then

$$\sum_{i=1}^{n}\left(y_i-\overline{y}\right)^2= \sum_{i=1}^{n}\left(y_i-\hat{y}_i \right)^2
+\sum_{i=1}^{n}\left( \hat{y}_i-\overline{y}\right)^2+2\sum_{i=1}^{n}\left(y_i-\hat{y}_i \right)\left( \hat{y}_i-\overline{y}\right)$$

- If the predicted values $\hat{y}_i$ are determined using the least-squares coefficients $\hat{\beta}_0$ and $\hat{\beta}_1$, then:

$$\sum_{i=1}^{n}\left(y_i-\hat{y}_i \right)\left( \hat{y}_i-\overline{y}\right)=0$$

---

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

- We then find the following relation between the sum of squares:

$$\underbrace{\sum_{i=1}^{n}\left(y_i-\overline{y}\right)^2}_{TSS}= \underbrace{\sum_{i=1}^{n}\left(y_i-\hat{y}_i \right)^2}_{RSS}
+\underbrace{\sum_{i=1}^{n}\left( \hat{y}_i-\overline{y}\right)^2}_{Regression SS}.$$

- 
  - TSS: Variability between the observed responses.
  - RSS: Variability comes from the error terms in the regression model.
  - Regression SS: Variability explained by the regression line.

---

### Correlation Coefficient $r$ and Determination Coefficient $R^2$

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

- **R-squared** or fraction of variance explained is

$$
R^2= \frac{\text{Regression SS}}{\text{TSS}} = \frac{\text{TSS-RSS}}{\text{TSS}} = 1 - \frac{\text{RSS}}{\text{TSS}}
$$

- If $R^2 \approx 1$:
  - The variability of the responses is almost completely explained by the regression line.
  - The variability around the regression line is small.
- It can be shown that in this simple linear regression setting that $R^2 = r^2$, where $r$ is the correlation between X and Y. 

Why?

---

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

```{r}
anova(slm)
```

RSS = 69793, Regression SS = 20036, TSS = 69793 + 20036

```{r}
R_2 = 20036/(69793 + 20036)
R_2
```

---

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

```{r}
summary(slm)
```

---

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

```{r}
r = cor(data_all$num_meds, data_all$days, method="pearson")
r^2
```

22% of the variability in the responses is explained by the regression line, the remaining 78% of the variability is caused by the error terms.

---

### Prediction Interval and Confidence Interval

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

- Prediction interval with confidence level $(1-\alpha)$

$$\left[\hat{y}-t_{n-2,1-\frac{\alpha}{2}}se\left(Y-\hat{Y}\right),
\hat{y}+t_{n-2,1-\frac{\alpha}{2}}se\left(Y-\hat{Y} \right) \right].$$
where $\hat{y}=\hat{\beta}_0 + \hat{\beta}_1x$
$$se\left(Y-\hat{Y} \right)=s\sqrt{1+\frac{1}{n}+\frac{(x-\overline{x})^2}{s^2_x(n-1)}}$$
(we replace the unknown $\sigma$ by the known $s$)

Why?

---

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

- 
  - Interval for the real response $Y$
  - Take into account the error term $\epsilon$ and the uncertainty of the regression coefficients.

- Confidence interval
  - Estimate $\hat{y}$ is an approximation for the expected response $\mathbb{E}[Y]$
  - The interval where we expect the expected response $\mathbb{E}[Y]$ will be.

$$\left[\hat{y}-t_{n-2,1-\frac{\alpha}{2}}se\left(\hat{Y} \right), \hat{y}-t_{n-2,1+\frac{\alpha}{2}}se\left(\hat{Y} \right) \right]$$
where 
$$se\left(\hat{Y} \right)=s\sqrt{\frac{1}{n}+\frac{(x-\overline{x})^2}{s^2_x(n-1)}}$$

---

### Model Diagnostic

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

- Investigate if the assumptions of the model are met by the data
  - Lack of independence:
      - The response variables $Y_1,Y_2,\ldots,Y_n$ have to be independent copies of $Y$.
      - As a result: the residuals $\epsilon_1,\epsilon_2,\ldots,\epsilon_n$ have to be independent.
  - Heteroscedasticity:
      - Each error term $\epsilon_i$ should have the same variance.
      - Heteroscedasticity = the variance of the error terms depends on the observations.
  - Relation between model deviations and explanatory variables:
      - The explanatory variables can help to explain deviations.
      - This information can be used to improve the model.

---

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

- Non-normal distribution:
      - Normality of the error terms is used to construct confidence intervals
      - The residuals are drawn from a normal distribution

---

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

- Unusual points:
      - Individual observations can have a big impact on the estimation of the model
      - The results (prediction/inference) can be sensitive to single observations
      - Investigate the sensitivity of the model on the extreme observations
      - Outlier: unusual observations in the $Y$ direction
        - Decide to keep the outlier in the data set: the observation actually happened and provides important information for the predictions
        - Decide to remove the outlier from the data set: the outlier is a wrong observation and not important for our particular study
      - High leverage point: unusual observation in the $X$ direction.
- There shouldn't be any pattern in the residuals

---

### Outliers and High-leverage points: Example

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

- The data set:
  - 19 basis points
  - outlier: $A$
  - high leverage point $C$
  - outlier & high leverage point: $B$

---

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

```{r}
#outliers and high leverage points
x1=c(1.5, 1.7, 2, 2.2, 2.5, 2.5, 2.7, 2.9, 3.0, 
     3.5, 3.8, 4.2, 4.3, 4.6, 4, 5.1, 5.1, 5.2, 5.5)
y1=c(3, 2.5, 3.5, 3.0, 3.1, 3.6, 3.2, 3.9, 4, 4, 
     4.2, 4.1, 4.8, 4.2, 5.1, 5.1, 5.1, 4.8, 5.3)
Ax=c(3.4)
Ay=c(8)
Bx=c(9.5)
By=c(8)
Cx=c(9.5)
Cy=c(2.5)
x=c(x1,Ax,Bx,Cx)
y=c(y1,Ay,By,Cy)
```

---


```{r}
plot(c(Ax,Bx,Cx), c(Ay,By,Cy), 
     xlim=c(0,20), ylim=c(0,20), 
     xlab="x-values", ylab="y-values")
text(c(Ax,Bx,Cx), c(Ay,By,Cy)-1, 
     c("A","B","C"))
points(x1,y1)
```

---

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

- We apply linear regression model in 3 different situations:
  - only the 19 basis points
  - the 19 basis points + the outlier $A$
  - the 19 + basis points + outlier/high leverage point $B$
  - the 19 basis points + the high leverage point $C$

---

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

```{r}
# Least squares estimate for the whole data set
Outliers.full.lm=lm(y~x)
summary(Outliers.full.lm)
```

---

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

```{r}
# Least squares with 19 basis points
Outliers.basis.lm=lm(y1~x1)
summary(Outliers.basis.lm)
```

---

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

```{r}
# Least squares wity basis points + A 
Outliers.A.lm=lm(c(y1,Ay)~c(x1,Ax))
summary(Outliers.A.lm)
```

---

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

```{r}
# Least squares with basis points + B 
Outliers.B.lm=lm(c(y1,By)~c(x1,Bx))
summary(Outliers.B.lm)
```

---

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

```{r}
# Least squares wity basis points + C 
Outliers.C.lm=lm(c(y1,Cy)~c(x1,Cx))
summary(Outliers.C.lm)
```

---


```{r}
# plot the results 
plot(c(Ax,Bx,Cx), c(Ay,By,Cy), xlim=c(0,20), ylim=c(0,20), 
     xlab="x-values", ylab="y-values")
text(c(Ax,Bx,Cx), c(Ay,By,Cy)-1, c("A","B","C"))
points(x1,y1)
abline(Outliers.basis.lm,col="red",lwd=2)
abline(Outliers.A.lm,col="blue",lwd=2)
abline(Outliers.B.lm,col="black",lwd=2)
abline(Outliers.C.lm,col="green",lwd=2)
legend("bottomright", c("19 basis points", " with A", "with B", "with C"), 
       col=c("red", "blue", "black", "green"), lty=c(1,1,1,1))
```


---

### Residual Analysis

<img align="right" src="image/University-Wordmark-Full-Color-CMYK.jpg" alt="fig1" width="200" height="500" /> 

- The **residual standard error**

$$
RSE = \sqrt{\frac{1}{n-2}RSS} = \sqrt{\frac{1}{n-2}\sum_{i=1}^{n}\left(y_i-{\color{red}{\hat{y}_i}} \right)^2}
$$

- RSE is an estimator for the volatility $\sigma^2$ of the error terms $\epsilon_i$ 
$$
RSE^2 \approx \sigma^2.
$$

- 
  - $n-2$: degrees of freedoms, consider $\hat\beta_0$ and $\hat\beta_1$
  - RSE is an unbiased estimator for $\sigma^2$
  - $\overline{e}=\frac{1}{n}\sum_{i=1}^{n}e_i=0$
  - $RSE^2 = \frac{\text{RSS}}{n-2}=\text{Mean Squared Error (MSE)}$


